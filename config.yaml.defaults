# Angry AI Configuration - DEFAULT TEMPLATE
# Copy this file to config.yaml and edit with your settings
#
# Generic AI code reviewer with build validation
# Works with ANY codebase - just configure your build command

# =============================================================================
# LLM Server Configuration
# =============================================================================
# Supports both vLLM and Ollama backends with automatic detection.
# Multiple hosts enable round-robin load balancing for higher throughput.
# Multiple models provide fallback if preferred model isn't available.
#
# KEY FEATURES:
#   - vLLM preferred over Ollama (faster inference)
#   - Automatic port expansion (just specify hostname)
#   - Model fallback chain (tries models in order until one works)
#   - Round-robin across multiple GPU servers
#
llm:
  # -------------------------------------------------------------------------
  # HOSTS: LLM server URLs
  # -------------------------------------------------------------------------
  # Just list your server hostnames - ports are handled automatically!
  #
  # AUTOMATIC PORT EXPANSION:
  #   "http://gpu-server" expands to try:
  #     1. http://gpu-server:8000  (vLLM default)
  #     2. http://gpu-server:11434 (Ollama default)
  #
  # The system probes each expanded URL and uses the first working backend.
  # vLLM is preferred over Ollama when both are available.
  #
  # EXAMPLES:
  #   Single server (most common):
  #     hosts:
  #       - "http://gpu-server"
  #
  #   Multiple servers for load balancing:
  #     hosts:
  #       - "http://gpu-server-1"
  #       - "http://gpu-server-2"
  #       - "http://gpu-server-3"
  #
  #   Explicit ports (if using non-standard ports):
  #     hosts:
  #       - "http://gpu-server:9000"  # Custom vLLM port
  #       - "http://gpu-server:12345" # Custom Ollama port
  #
  hosts:
    - "http://localhost"
  
  # -------------------------------------------------------------------------
  # MODELS: Ordered list of models to try (first available wins)
  # -------------------------------------------------------------------------
  # Each host will use the first model from this list that it has loaded.
  # This allows graceful fallback when preferred models aren't available.
  #
  # Model names depend on your backend:
  #   - vLLM: Usually the HuggingFace model path or custom name
  #   - Ollama: Use "ollama list" to see available models (name:tag format)
  #
  # TIP: Run the tool with --validate-only to see which models are available
  #      on each host before configuring this list.
  #
  models:
    - "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8"   # Preferred for vLLM (HuggingFace name)
    - "qwen2.5-coder:32b"                           # Good Ollama fallback
    # - "qwen2.5-coder:14b"                         # Smaller fallback
    # - "codellama:34b"                             # Alternative
  
  # Request timeout in seconds
  # Increase if you get timeouts with large files or slow models
  timeout: 600
  
  # Maximum tokens to generate per response
  max_tokens: 4096
  
  # Temperature for generation (0.0 = deterministic, 1.0 = creative)
  temperature: 0.1

  # Interval in seconds to print /api/ps stats while a model request is running.
  # Set to 0 to disable live GPU utilization updates (recommended - too verbose).
  # Note: Only works with Ollama backend
  ps_monitor_interval: 0

  # Batching configuration (primarily for Ollama backend)
  # Set num_batch to a fixed value to force a specific micro-batch size.
  # Leave num_batch null and enable adaptive to let Angry AI scale batch size
  # based on prompt length.
  batching:
    num_batch: null          # e.g., 8 to force a fixed size
    adaptive: true           # auto-scale num_batch when fixed value not set
    min_chars: 8000          # prompt length where batching starts ramping up
    max_chars: 60000         # prompt length that maps to max_num_batch
    min_num_batch: 2         # minimum batch size when adaptive is enabled
    max_num_batch: 12        # maximum batch size Angry AI will request
    # Maximum parallel LLM requests across all hosts
    # Set to 0 for DYNAMIC mode: automatically determined from server metrics
    #   (queries vLLM /metrics or Ollama /api/ps for KV cache usage)
    # Set to 1+ for STATIC mode: fixed cap on concurrent requests
    #   (will warn if this differs significantly from server-recommended value)
    max_parallel_requests: 0

  # Additional per-request options forwarded directly to the LLM API.
  # For Ollama: num_ctx, num_gpu, repeat penalties, etc.
  # For vLLM: top_p, presence_penalty, frequency_penalty, etc.
  # Example:
  # options:
  #   num_ctx: 8192
  #   top_p: 0.95
  options: {}

  # Health check configuration for automatic host recovery
  # When a host fails, it's moved to an unhealthy pool and the system
  # continues with remaining hosts. Background health checks attempt
  # to reconnect using exponential backoff (30s, 60s, 120s, 240s, 300s max).
  # Once recovered, the host is automatically restored to the active pool.
  health_check:
    enabled: true        # Enable automatic recovery (set false for legacy behavior)
    interval: 30         # Base retry interval in seconds
    max_interval: 300    # Maximum retry interval (5 minutes)
    timeout: 10          # Timeout for health check probes

# =============================================================================
# Legacy Ollama Configuration (DEPRECATED - use 'llm' section above)
# =============================================================================
# This section is kept for backward compatibility. If both 'llm' and 'ollama'
# sections exist, 'llm' takes precedence. New configurations should use 'llm'.
#
# ollama:
#   url: "http://localhost:11434"
#   model: "qwen2.5-coder:32b"
#   timeout: 600
#   max_tokens: 4096
#   temperature: 0.1

# Source tree configuration
source:
  # Path to source code root directory
  # (Relative paths resolved from config file location)
  # Default ".." assumes config is in a subdirectory like angry-ai/
  root: ".."
  
  # Build command to validate ALL changes in current directory
  # This is the ONLY project-specific configuration!
  # The script chdir's to source root before running this command.
  #
  # Examples for different projects:
  #   FreeBSD:     "sudo make -j$(sysctl -n hw.ncpu) buildworld"
  #   Linux:       "make -j$(nproc)"
  #   CMake:       "cmake --build build -j$(nproc)"
  #   Autotools:   "./configure && make -j$(nproc)"
  #   Rust:        "cargo build --release"
  #   Go:          "go build ./..."
  #   Python:      "python -m pytest" or "tox"
  #   Node.js:     "npm test"
  build_command: "sudo make -j$(sysctl -n hw.ncpu) buildworld"
  
  # Build timeout in seconds (adjust for your project)
  # FreeBSD buildworld: ~2 hours
  # Linux kernel: ~30 minutes
  # Most projects: ~5-10 minutes
  build_timeout: 7200  # 2 hours
  
  # Optional: branch to use for review operations (defaults to repo default)
  # If this branch is checked out in another worktree, a reviewer/* branch will be created
  # branch: "main"

  # Pre-build command to run once at startup
  # Examples: "sudo -v" (cache sudo), "npm install" (deps), "" (none)
  # Set to empty string to disable
  pre_build_command: "sudo -v"

# Review configuration
review:
  # File chunking settings (prevents timeouts on large files)
  # Files larger than chunk_threshold get split into chunks
  chunk_threshold: 400          # Lines before file is chunked (default: 400)
  chunk_size: 250               # Maximum lines per chunk (default: 250)
  
  # Agent directory (contains agent behavior configuration)
  #
  # AGENT SPEC FORMAT (preferred):
  #   Structure: personas/<name>/
  #     - agent.yaml (Agent Spec configuration - all settings in one file)
  #   See: https://oracle.github.io/agent-spec/26.1.0/
  #
  # LEGACY FORMAT (still supported):
  #   Structure: personas/<name>/
  #     - AI_START_HERE.md (bootstrap/instructions)
  #     - PERSONA.md (personality definition)
  #     - HANDOVER.md (workflow protocol)
  #
  # Available agents:
  #   - freebsd-angry-ai: Ruthless security auditor (default, battle-tested)
  #   - security-hawk: Paranoid security specialist
  #   - performance-cop: Speed optimization expert
  #   - friendly-mentor: Educational, encouraging reviewer
  #   - example: Balanced, educational template
  #
  # Benefits:
  #   - Source tree stays clean (only code changes)
  #   - Multiple agents (security-focused, performance, friendly)
  #   - Portable agents (share/version separately)
  #   - Agent Spec format is standardized and interoperable
  persona: "personas/freebsd-angry-ai"
  
  # Target number of directories to complete per session
  # Session continues until this many directories are successfully reviewed,
  # built, committed and pushed. Use 0 for unlimited (run until HALT or error).
  target_directories: 10
  
  # Maximum iterations per directory before giving up and moving on
  # Prevents infinite loops on problematic directories
  # A typical directory needs 50-100 iterations (read files, edit, build)
  max_iterations_per_directory: 200
  
  # Parallel file processing
  # Controls how many files are reviewed concurrently within a directory.
  # Set to 0 for DYNAMIC mode: automatically determined from server metrics
  #   (queries vLLM /metrics or Ollama /api/ps for GPU/KV cache capacity)
  # Set to 1 for SEQUENTIAL mode: review files one at a time (safest)
  # Set to 2+ for STATIC parallel mode: fixed number of concurrent reviews
  #   (will warn at startup if this differs from server-recommended value)
  # Note: Builds are always sequential regardless of this setting
  max_parallel_files: 0

  # Performance optimization features
  # These settings enable parallelization and connection pooling for faster reviews
  performance:
    # Phase 1: Enable per-file locking for parallel edit application
    # When true, edits to different files are applied in parallel (2-3x speedup)
    # When false, all edits are applied sequentially with a global lock
    parallel_edits: true

    # Phase 2: Use HTTP connection pooling for API requests
    # When true, HTTP connections are reused across requests (10-20% latency reduction)
    # When false, each request creates a new TCP connection (legacy behavior)
    connection_pooling: true

    # Maximum concurrent HTTP connections in the connection pool
    # Only used when connection_pooling is enabled
    max_http_connections: 16

    # Phase 3: Extend parallel review to more contexts beyond SET_SCOPE
    # When true, parallel review is used for batched file operations (1.5-2x throughput)
    # When false, parallel review only used in SET_SCOPE action
    aggressive_parallelism: true

    # Phase 4: EXPERIMENTAL - Background build execution (HIGH RISK)
    # When true, builds run in background while review continues (2-5x for build-heavy workflows)
    # When false, builds block all review work (default, safe behavior)
    # WARNING: This feature is experimental and may cause race conditions
    # Only enable if you understand the risks and have tested thoroughly
    background_builds: false

  # Maximum commits to revert during pre-flight sanity check recovery
  # If source doesn't build at startup, pre-flight reverts commits until it does
  # Higher values allow recovery from longer periods of broken commits
  # Default: 100 (can handle ~100 bad commits before giving up)
  max_reverts: 100
  
  # Files/patterns to skip during review
  skip_patterns:
    - "*.o"
    - "*.a"
    - "*.so"
    - ".git/*"
    - "angry-ai/*"
    - "ai-code-reviewer/*"  # Prevent self-review!
    - "*.pyc"
    - "__pycache__/*"

# Logging configuration
logging:
  # Directory for log files (relative to source root)
  log_dir: ".ai-code-reviewer/logs"
  
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Keep last N log files per session
  max_log_files: 100

# Internal operations logging (for debugging and improving the tool)
ops_logging:
  # Directory for operations log (relative to cwd, gitignored)
  log_dir: ".reviewer-log"
  
  # Optional: sync logs to a dedicated branch in the audited project
  # This makes logs available for analysis without polluting main branch
  # Set to null/empty to disable branch sync
  # sync_to_branch: "reviewer-logs"

