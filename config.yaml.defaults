# Angry AI Configuration - DEFAULT TEMPLATE
# Copy this file to config.yaml and edit with your settings
#
# Generic AI code reviewer with build validation
# Works with ANY codebase - just configure your build command

# =============================================================================
# LLM Server Configuration (NEW - supports vLLM + Ollama with multi-host)
# =============================================================================
# The 'llm' section supports multiple hosts and models for increased throughput.
# For each host, vLLM is tried first (OpenAI-compatible API), then Ollama.
# Models are tried in priority order until one is found on the server.
#
# Benefits:
#   - Round-robin load balancing across multiple GPU servers
#   - Automatic backend detection (vLLM preferred, Ollama fallback)
#   - Model priority list for graceful fallback
#
llm:
  # List of LLM server URLs (at least one required)
  # Requests are distributed round-robin across all available hosts
  # Each host is probed at startup: vLLM API first, then Ollama API
  #
  # IMPORTANT: vLLM and Ollama use different ports!
  #   - vLLM default port: 8000 (OpenAI-compatible API)
  #   - Ollama default port: 11434
  #
  # List vLLM hosts FIRST (preferred), then Ollama hosts (fallback):
  hosts:
    - "http://localhost:8000"      # vLLM server (tried first)
    - "http://localhost:11434"     # Ollama server (fallback)
    # - "http://gpu-server-2:8000"   # Additional vLLM server
    # - "http://gpu-server-2:11434"  # Additional Ollama server
  
  # List of models in priority order (first available is used)
  # Each host will use the first model from this list that it has available
  # This allows graceful fallback when preferred models aren't available
  models:
    - "NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"
    - "qwen2.5-coder:32b"
    # - "qwen2.5-coder:14b"    # Fallback if 32b not available
    # - "codellama:34b"        # Another fallback option
  
  # Request timeout in seconds
  # Increase if you get timeouts with large files or slow models
  timeout: 600
  
  # Maximum tokens to generate per response
  max_tokens: 4096
  
  # Temperature for generation (0.0 = deterministic, 1.0 = creative)
  temperature: 0.1

  # Interval in seconds to print /api/ps stats while a model request is running.
  # Set to 0 to disable live GPU utilization updates (recommended - too verbose).
  # Note: Only works with Ollama backend
  ps_monitor_interval: 0

  # Batching configuration (primarily for Ollama backend)
  # Set num_batch to a fixed value to force a specific micro-batch size.
  # Leave num_batch null and enable adaptive to let Angry AI scale batch size
  # based on prompt length.
  batching:
    num_batch: null          # e.g., 8 to force a fixed size
    adaptive: true           # auto-scale num_batch when fixed value not set
    min_chars: 8000          # prompt length where batching starts ramping up
    max_chars: 60000         # prompt length that maps to max_num_batch
    min_num_batch: 2         # minimum batch size when adaptive is enabled
    max_num_batch: 12        # maximum batch size Angry AI will request
    max_parallel_requests: 4 # cap on simultaneous LLM requests across all hosts

  # Additional per-request options forwarded directly to the LLM API.
  # For Ollama: num_ctx, num_gpu, repeat penalties, etc.
  # For vLLM: top_p, presence_penalty, frequency_penalty, etc.
  # Example:
  # options:
  #   num_ctx: 8192
  #   top_p: 0.95
  options: {}

# =============================================================================
# Legacy Ollama Configuration (DEPRECATED - use 'llm' section above)
# =============================================================================
# This section is kept for backward compatibility. If both 'llm' and 'ollama'
# sections exist, 'llm' takes precedence. New configurations should use 'llm'.
#
# ollama:
#   url: "http://localhost:11434"
#   model: "qwen2.5-coder:32b"
#   timeout: 600
#   max_tokens: 4096
#   temperature: 0.1

# Source tree configuration
source:
  # Path to source code root directory
  # (Relative paths resolved from config file location)
  # Default ".." assumes config is in a subdirectory like angry-ai/
  root: ".."
  
  # Build command to validate ALL changes in current directory
  # This is the ONLY project-specific configuration!
  # The script chdir's to source root before running this command.
  #
  # Examples for different projects:
  #   FreeBSD:     "sudo make -j$(sysctl -n hw.ncpu) buildworld"
  #   Linux:       "make -j$(nproc)"
  #   CMake:       "cmake --build build -j$(nproc)"
  #   Autotools:   "./configure && make -j$(nproc)"
  #   Rust:        "cargo build --release"
  #   Go:          "go build ./..."
  #   Python:      "python -m pytest" or "tox"
  #   Node.js:     "npm test"
  build_command: "sudo make -j$(sysctl -n hw.ncpu) buildworld"
  
  # Build timeout in seconds (adjust for your project)
  # FreeBSD buildworld: ~2 hours
  # Linux kernel: ~30 minutes
  # Most projects: ~5-10 minutes
  build_timeout: 7200  # 2 hours
  
  # Pre-build command to run once at startup
  # Examples: "sudo -v" (cache sudo), "npm install" (deps), "" (none)
  # Set to empty string to disable
  pre_build_command: "sudo -v"

# Review configuration
review:
  # File chunking settings (prevents timeouts on large files)
  # Files larger than chunk_threshold get split into chunks
  chunk_threshold: 400          # Lines before file is chunked (default: 400)
  chunk_size: 250               # Maximum lines per chunk (default: 250)
  
  # Persona directory (contains AI behavior, lessons, progress tracking)
  # Structure: personas/<name>/
  #   - AI_START_HERE.md (bootstrap/instructions)
  #   - PERSONA.md (personality definition)
  #   - LESSONS.md (learned bug patterns)
  #   - HANDOVER.md (workflow protocol)
  #   - REVIEW-SUMMARY.md (progress tracking)
  #   - AGENTS.md, @AGENTS.md (agent hints)
  #
  # Available personas:
  #   - freebsd-angry-ai: Ruthless security auditor (default, battle-tested)
  #   - example: Balanced, educational template
  #
  # Benefits:
  #   - Source tree stays clean (only code changes)
  #   - Multiple personas (security-focused, performance, friendly)
  #   - Portable personas (share/version separately)
  persona: "personas/freebsd-angry-ai"
  
  # Target number of directories to complete per session
  # Session continues until this many directories are successfully reviewed,
  # built, committed and pushed. Use 0 for unlimited (run until HALT or error).
  target_directories: 10
  
  # Maximum iterations per directory before giving up and moving on
  # Prevents infinite loops on problematic directories
  # A typical directory needs 50-100 iterations (read files, edit, build)
  max_iterations_per_directory: 200
  
  # Parallel file processing (EXPERIMENTAL)
  # Process multiple files concurrently for faster reviews
  # Set to 1 for sequential processing (safest)
  # Set to 2-4 for parallel processing (faster but more complex)
  # Note: Builds are always sequential per directory regardless of this setting
  # DEFAULT: 2 (moderate parallelism for testing)
  max_parallel_files: 2
  
  # Maximum commits to revert during pre-flight sanity check recovery
  # If source doesn't build at startup, pre-flight reverts commits until it does
  # Higher values allow recovery from longer periods of broken commits
  # Default: 100 (can handle ~100 bad commits before giving up)
  max_reverts: 100
  
  # Files/patterns to skip during review
  skip_patterns:
    - "*.o"
    - "*.a"
    - "*.so"
    - ".git/*"
    - "angry-ai/*"
    - "ai-code-reviewer/*"  # Prevent self-review!
    - "*.pyc"
    - "__pycache__/*"

# Logging configuration
logging:
  # Directory for log files (relative to source root)
  log_dir: ".ai-code-reviewer/logs"
  
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Keep last N log files per session
  max_log_files: 100

# Internal operations logging (for debugging and improving the tool)
ops_logging:
  # Directory for operations log (relative to cwd, gitignored)
  log_dir: ".reviewer-log"
  
  # Optional: sync logs to a dedicated branch in the audited project
  # This makes logs available for analysis without polluting main branch
  # Set to null/empty to disable branch sync
  # sync_to_branch: "reviewer-logs"

